{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0110c01",
   "metadata": {},
   "source": [
    "# Dissertation Data Analysis Notebook \n",
    "\n",
    "**This notebook contains the workflow for my dissertation on the control of folded geology on drainage network patterns.**\n",
    "\n",
    "* Author: Rutger Vervoordeldonk\n",
    "* Last updated: 07/02/2024\n",
    "\n",
    "This notebook provides the full workflow of my dissertation data analysis. It uses:\n",
    "1. `HydroBasins` datasets to select river basins\n",
    "2. `LSDTopoTools` to extract river networks from Copernicus 30m resolution Digital Elevation Models (accessed through the `OpenTopography` API).\n",
    "3. `rivernetworkpy` (`rnp`): a (local) python package that contains the data analysis scripts for this notebook\n",
    "4. `WVGES Geological Data` to  compare orientations of network segments to geological structures\n",
    "5. `HydroLakes` datatasets to remove sections of river networks where there are lakes\n",
    "\n",
    "The data analysis can be subdivided into roughly two parts:\n",
    "* Part 1: The Segment Orientation Analysis with Bedding Data (SOA-BED). \n",
    "* Part 2: The Segment Orientation Analysis w/o Bedding Data (SOA) and the Junction Geometry Analysis (JGA) \n",
    "The analysis is divided this way on the basis that Part 1 requires networks to be extracted slightly differently than in Part 2. This will become clear in the individual Parts. \n",
    "\n",
    "The final output of this analysis are\n",
    "* SOA/SOA-BED plots: Half-dial plots showing segment orientation distributions. This plot uses data generated both in Part 1 and Part 2. In addition, also a Test for Uniformity, Unimodality, Bimodality of Segment orientations will be performed\n",
    "* JGA plots: Junction Geometry Kuipers Boxplots, Junction Geometry CDF plots and $A_R$ CDF plots. Draws only upon data of Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cd4708",
   "metadata": {},
   "source": [
    "## Setup file directory\n",
    "\n",
    "(Manually) setup a directory with the following structure:\n",
    "\n",
    "- **basins**: _an empty subdirectory that will be used to store the data of individual basins that are downloaded and analysed. Includes sub-subdirectories **PART1** and **PART2**_\n",
    "- **hydrobasins**: _a subdirectory with shapefiles of the levels 5, 6 and 7 of the Central and North America HydroBasins (downloadable from their website). Also useful to download HydroRIVERS of the same area to this folder; HydroRIVERS is not necessary for the analysis but can come in handy when picking HydroBASINS for analysis._\n",
    "- **hydrolakes**: _a subdirectory with shapefiles of hydrolakes of North America. Also downloadable from their website. You should then use QGIS to remove all lakes outside the area you are interested in to save a lot of computation time later._\n",
    "- **rivernetworkpy-base**: _a subdirectory containing the `rivernetworkpy` package_\n",
    "- **wvbed**: _a subdirectory containing the West Virginia bedding measurements shapefile AND the shapefile of corresponding quadrangles_\n",
    "- **vrpapoutlines** _a subdirectory containing two shapefiles, one for the Valley and Ridge and one for the Appalachian Plateau, describing the outline of the two physiographic provinces. I derived those from Fennemans map of provinces downloadable from the USGS, as described in my dissertation. I can email them to you if you want them. \n",
    "- my_OT_api_key: _a txt file only containing your API key from Opentopography.org. To get an API key you have to make an account there._\n",
    "- a QGIS (or ArcGIS) file: _useful to open and inspect the WV_BED shapefile and HydroBasins layers, plus any geospatial data generated during this workflow_\n",
    "- this notebook\n",
    "\n",
    "Later, this notebook will generate additional subdirectories for result data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ca16f",
   "metadata": {},
   "source": [
    "## Setup `rivernetworkpy`\n",
    "\n",
    "To setup `rivernetworkpy`:\n",
    "1. make sure that you are in the right conda environment \n",
    "2. using the terminal, cd yourself to the rivernetworkpy-base directory: `cd your-path/rivernetworkpy-base`\n",
    "3. develop the local python package: `python3 setup.py develop`\n",
    "\n",
    "Luckily, you only have to do this once! If you were to change something to `rivernetworkpy`, just restart the kernel of this notebook and re-import the package - there's no need to re-develop the package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529d11ad",
   "metadata": {},
   "source": [
    "## Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07edcc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lsdviztools.lsdmapwrappers as lsdmw  # lsdmw is a python driver for LSDTopoTools command line tools!\n",
    "import pandas as pd # to read csv's \n",
    "import os # needed later to create subdirectories in the basins folder\n",
    "import rivernetworkpy as rnp # rnp contains all data-analysis scripts for this workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d613a1e",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07ae16c",
   "metadata": {},
   "source": [
    "## Select HydroBasins for analysis\n",
    "\n",
    "Select the HydroBasins for analaysis by entering HYBAS_IDs and intuitive nicknames (only letters, numbers and underscores!) in the dictionary below. HYBAS_IDs can be inspected using QGIS. \n",
    "\n",
    "By default, the dictionary contains the hydrobasins that I used for the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dca01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "part1_hybas_ids = {\n",
    "    7070547240 : \"WV_VR_West\",\n",
    "    7050041400 : \"WV_VR_North\",\n",
    "    7050569090 : \"WV_VR_South\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dd76ee",
   "metadata": {},
   "source": [
    "## Setup folders for selected HydroBasins and download DEMs\n",
    "\n",
    "For a given HydroBasin, the `setup_directory_with_DEM` function will setup a folder in the subdirectory **basins** and download a DEM using `LSDTopoTools`' OpenTopography Scraper. \n",
    "\n",
    "Note that, if a folder already exists with the same nickname, no new folder will be set up and no DEM will be downloaded for this hydrobasin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33ddb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hybas_id in part1_hybas_ids:\n",
    "    print(\"------------NOW STARTING WITH: \"+part1_hybas_ids[hybas_id]+\"------------------\")\n",
    "    rnp.setup_directory_with_DEM(hybas_id, part1_hybas_ids[hybas_id], part = \"PART1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8039c3-0a0a-4160-9c69-852b2f2a4a3f",
   "metadata": {},
   "source": [
    "## Perform flow routing and calculate segment bearings for each basin using `LSDTopoTools`\n",
    "Note that the function `run_lsdtt_for_basin` is defined here and not in `rivernetworkpy` to make it a bit easier to adjust lsdtt parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3383c56-36b0-44e2-a255-5e1ab0588056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lsdviztools.lsdmapwrappers as lsdmw\n",
    "    \n",
    "def run_lsdtt_for_basin(nickname, part):\n",
    "    Dataset_prefix = nickname\n",
    "    source_name = \"COP30\"\n",
    "    DataDirectory = \"./Basins/\"+part+\"/\"+Dataset_prefix+\"/\"\n",
    "    \n",
    "    lsdtt_parameters = {\"print_segment_bearings_and_gradients_to_csv\" : \"true\",\n",
    "                        \"SA_vertical_interval\" : \"10\",\n",
    "                        \"threshold_contributing_pixels\" : \"500\", \n",
    "                        \"print_chi_data_maps\":\"true\",\n",
    "                        \"write_hillshade\" : \"true\",\n",
    "                        \"convert_csv_to_geojson\":\"false\",\n",
    "                        \"only_take_largest_basin\":\"true\",\n",
    "                        \"maximum_basin_size_pixels\":\"10000000000000\",\n",
    "                        \"print_basin_raster\":\"true\"}\n",
    "\n",
    "    r_prefix = Dataset_prefix+\"_\"+source_name+\"_UTM_clipped\"\n",
    "    w_prefix = Dataset_prefix+\"_\"+source_name+\"_UTM\"\n",
    "\n",
    "    lsdtt_drive = lsdmw.lsdtt_driver(command_line_tool = \"lsdtt-basic-metrics\",\n",
    "                                     read_prefix = r_prefix,\n",
    "                                     write_prefix= w_prefix,\n",
    "                                     read_path = DataDirectory,\n",
    "                                     write_path = DataDirectory,\n",
    "                                     parameter_dictionary=lsdtt_parameters)\n",
    "    lsdtt_drive.print_parameters()\n",
    "    lsdtt_drive.run_lsdtt_command_line_tool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f90b672-0494-44c7-8db3-a260a316c6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hybas_id in part1_hybas_ids:\n",
    "    print(\"------------NOW STARTING WITH: \"+part1_hybas_ids[hybas_id]+\"------------------\")\n",
    "    run_lsdtt_for_basin(part1_hybas_ids[hybas_id], part = \"PART1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bd626d-e461-431f-bb07-d7c2e35697c7",
   "metadata": {},
   "source": [
    "There now should be a `chi_data_map.csv` file that containing the channel network in every folder, as well as `csv` files containing the segments and their bearings (called `_segment_bearings.csv`).\n",
    "\n",
    "The segment bearings csv contains all bearings within the DEM (so also those outside the basin we are interested in (the largest basin), on the edge of the DEM). \n",
    "\n",
    "This can be fixed through some merging of the segment bearings csv with the chi data map (the chi data map is only of the largest basin), based on lat/lons of the network's nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ac7682-fd12-4c92-8ef4-14eaa05a9436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for hybas_id in part1_hybas_ids:\n",
    "    print(\"------------NOW STARTING WITH: \"+part1_hybas_ids[hybas_id]+\"------------------\")\n",
    "    rnp.merge_segment_bearings_with_chi_data_map_for_basin(part1_hybas_ids[hybas_id], part = \"PART1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d48ebd-4b48-471f-859d-29e118accd40",
   "metadata": {},
   "source": [
    "## Find bedding measurements along channels\n",
    "\n",
    "Now, we are going to run for each basin a function that finds all bedding measurements within a 1km radius of all along-stream segments of a network that are inside the VRP Quadrangles with bedding measurements and outside lakes. \n",
    "\n",
    "In order to generate this dataset, the function sequentially:\n",
    "* Obtains along-stream segments as line data from the chi data map using junctions contained in the segment_bearings_dataset. \n",
    "* writes out a GeoJSON (shape) file containing the lines of the river network. This should be manually inspected in QGIS to verify that all the points were correctly converted into lines.\n",
    "* discards of all lines that are not fully contained within the `WV_QUADS_VRP.shp` polygon\n",
    "* also discards of all lines that lie (partially or fully) inside a `HydroLAKES` polygon. **It is advisable to restrict the spatial extent of the HydroLAKES dataset to NE USA before running this code, otherwise it will take forever!**\n",
    "* writes out another GeoJSON file with the lines, such that you can check that the previous two steps went correctly\n",
    "* buffers the remaining lines by a radius of 1000m\n",
    "* identifies for each buffered line segment all bedding measuruments (`WV_BED_VRP.shp`) that lie within the buffer (**this step takes a long time!**)\n",
    "* writes out, to each PART1 basin folder, a csv ending in `_segment_bearings_bedding_measurements.csv`\n",
    "* writes out, to the PART1 basin folder, as csv containing all unique bedding measurements found within all the buffers. This is to, later, calculate means of strike and dip and also the total number of linked bedding measurements. Note that you would not be able to get unique values from just loping over all the bedding measurements in `_segment_bearings_bedding_measurements.csv` as that would lead to double values (1 bed measurement can be connected to multiple streams)!\n",
    "  \n",
    "The result of this function is a csv file ending in `_bedding_measurements.csv`. In the next step we will match the bedding measurements to the `_segment_bearings.csv` by matching the junction numbers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8945a6-2529-414f-b83b-2623eccb446a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for hybas_id in part1_hybas_ids:\n",
    "    print(\"------------NOW STARTING WITH: \"+part1_hybas_ids[hybas_id]+\"------------------\")\n",
    "    rnp.add_bedding_measurements_to_segments(part1_hybas_ids[hybas_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdc2bf9-c9a3-4a40-b3ac-675254692380",
   "metadata": {},
   "source": [
    "The next function will link add the bedding measurements to the `_segment_bearings.csv`, by merging based on junction numbers. \n",
    "\n",
    "The following columns are added to the `_segment_bearings.csv`:\n",
    "* azimuths, dips, dip_directions: the 'raw' data from the WVGES bedding measurements (directed data)\n",
    "* strikes, dip_orientations: the strikes and dip orientations derived from the 'raw' data, reduced to the interval [0, 180) (oriented data)\n",
    "* n_strike, n_dip_orientation: number of strike and dip orientation measurements (should be the same)\n",
    "* mean_strike, mean_dip_orientation: means of strike and dip orientation (used to calculate difference with segment orientation)\n",
    "* circ_var_strike, circ_var_dip_orientations: circular variance of strikes and dip orientations (not used but just there for fun)\n",
    "* segment_orientation: the segment bearing with directionality removed, such that all data on interval [0,180) (oriented data)\n",
    "* difference_segment_orientation_and_mean_strike (on interval [-90, 90])\n",
    "* difference_segment_orientation_and_mean_dip_orientation (on interval [-90, 90])\n",
    "\n",
    "The result is saved as a csv ending in `_segment_bearings_bedding_stats.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a826e75c-4d39-427e-a302-7febb6c6c8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hybas_id in part1_hybas_ids:\n",
    "    print(\"------------NOW STARTING WITH: \"+part1_hybas_ids[hybas_id]+\"------------------\")\n",
    "    rnp.add_bedding_measurements_to_segment_bearings(part1_hybas_ids[hybas_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05372c06-1109-4311-bf80-eed9a3e493d8",
   "metadata": {},
   "source": [
    "Next up, we are going to merge the `segment_bearings_bedding_stats.csv` and the `_unique_bedding_measurements.csv` files of the individual basins and save these in the `PART1` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f4abb-528b-44ff-b9fd-ae053a8b9572",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnp.merge_segment_bearings_bedding_stats(part1_hybas_ids)\n",
    "rnp.merge_unique_bedding_measurements(part1_hybas_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4e2cc2-9044-4cf7-8562-874964b59d49",
   "metadata": {},
   "source": [
    "We now have now prepared the dataset of river networks connected to bedding measurements. We will analyse this data later. We are first going to extract basins for Part 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a04902",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b0292c-44cb-4a8b-9f10-1a267e27a839",
   "metadata": {},
   "source": [
    "## Setup directories in PART2 and Download DEMs \n",
    "The procedure here is slightly different to Part 1. In Part 1 we wanted to find _all channels_ fully within the extent of VRP bedding measurements, thus also streams whose basins lie partially outside this extent.\n",
    "\n",
    "In Part 2, we want to find _as many basins of similar size_ that lie fully wihtin the extent of either the VRP or the AP.\n",
    "\n",
    "It would be easiest to download one DEM with the extent of the VRP polygon and one with the extent of the AP polygon, and then use LSDTopoTools `find_basins` function to find similarly sized basins. \n",
    "\n",
    "These DEMs would, however, be huge (>1 GB) and therefore require lots of memory when flow routing. The VRP and AP polygons must therefore be sensibly subdivided (i.e. along water divides) first. \n",
    "\n",
    "This will be done as follows:\n",
    "* First make a dictionary where the key represents a VRP or AP subdivision, then the values are Hydrobasin IDs within this subdivision.\n",
    "* Then, for each subdivision, collapse the Hydrobasin outlines into one geometry, buffer this by 1000m (buffer is necessary because Hydrobasins water divides - derived from 90m resolution DEMs - are expected to be slighlty off from those derived from 30m using LSDTopoTools), and find the intersection of this buffered polygon and the VRP or AP outline.\n",
    "* The result is polygons that represent sensible subdivisions of the VRP and AP.\n",
    "\n",
    "Later, we can download DEMS according to these polygons and perfrom flow routing, and later merge all the basins found together into one big CSV containing all data of the basins!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbac40ab-5473-4396-85f0-18b2eed166b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdivisions = {\n",
    "    \"VRP_SOUTH\": [7060597760],\n",
    "    \"VRP_MID_S\": [7070597680, 7070594640, 7070594650, 7070598130, 7070598140, 7070611080],\n",
    "    \"VRP_MID_N\": [7050041400],\n",
    "    \"VRP_NORTH\": [7050041050, 7060040360],\n",
    "    \"AP_NORTH\": [7060529340, 7060519270],\n",
    "    \"AP_MID_N\": [7060529460, 7060523200],\n",
    "    \"AP_MID_M\": [7060555510, 7060555650, 7060558980, 7060558770, 7060568950],\n",
    "    \"AP_MID_S\": [7060569090, 7060580820, 7060597680, 7060585550, 7060579690, 7060580930, 7060579680, 7060585700, 7070082250, 7070598400, 7060597760], \n",
    "    \"AP_SOUTH\": [7060571990, 7060571810, 7060562310, 7060572890]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23b55d2-d5de-4934-ad8f-0277a087b067",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import rivernetworkpy as rnp\n",
    "rnp.setup_directories_with_subdivision_outlines(subdivisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9c8121-24e4-4ff2-af77-49f84bfbfdc4",
   "metadata": {},
   "source": [
    "The above function set up file directories of the subdivisions and put a GeoJSON file of the outline in each. \n",
    "\n",
    "Checking in QGIS, you can see that all the outlines are correct except `AP_NORTH` and `AP_MID_N`. These subdivisions have a 'gap' running through the middle, which is due to the two regions of the AP not properly aligning on the small scale in the Physiographic Provinces dataset. It is very hard to remove the gap with code, so, before continuing, go to QGIS, load these two GeoJSONs and for each layer click 'Toggle Editing' and use the 'Vertex Tool' to remove the nodes that make up the gaps.\n",
    "\n",
    "Once that is done, run the next function that for each subdivision downloads the right DEM. That will take a while (depending on your internet connection) so make yourself lunch in the meantime. Dont forget to close the GeoJSONs you just opened (and adjusted), because otherwise the next thing won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cd1e5f-94a2-439c-a8d9-4548c58ce740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rivernetworkpy as rnp\n",
    "rnp.prepare_DEMS_for_subdivision_outlines(subdivisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c2aa61-ad7f-40cf-a739-1c694bc6e613",
   "metadata": {},
   "source": [
    "## Use `LSDTopoTools` to perform flow routing, find basins and get segment bearings and junction angles\n",
    "We first redefine the run_lsdtt_for_basin function to include \"get junction angles\" and \"find basins\", and remove \"only_take_largest_basin\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ed82db-60bb-4e1c-82c9-537b92157a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lsdviztools.lsdmapwrappers as lsdmw\n",
    "    \n",
    "def run_lsdtt_for_basin(nickname, part):\n",
    "    Dataset_prefix = nickname\n",
    "    source_name = \"COP30\"\n",
    "    DataDirectory = \"./Basins/\"+part+\"/\"+Dataset_prefix+\"/\"\n",
    "    \n",
    "    lsdtt_parameters = {\"print_segment_bearings_and_gradients_to_csv\" : \"true\",\n",
    "                        \"print_junction_angles_to_csv_in_basins\":\"true\",\n",
    "                        \"SA_vertical_interval\" : \"10\",\n",
    "                        \"threshold_contributing_pixels\" : \"500\", \n",
    "                        \"print_chi_data_maps\":\"true\",\n",
    "                        \"write_hillshade\" : \"true\",\n",
    "                        \"convert_csv_to_geojson\":\"false\",\n",
    "                        \"only_take_largest_basin\":\"false\",\n",
    "                        \"find_basins\":\"true\",\n",
    "                        \"maximum_basin_size_pixels\":\"10000000000000\",\n",
    "                        \"print_basin_raster\":\"true\"}\n",
    "\n",
    "    r_prefix = Dataset_prefix+\"_\"+source_name+\"_UTM_clipped\"\n",
    "    w_prefix = Dataset_prefix+\"_\"+source_name+\"_UTM\"\n",
    "\n",
    "    lsdtt_drive = lsdmw.lsdtt_driver(command_line_tool = \"lsdtt-basic-metrics\",\n",
    "                                     read_prefix = r_prefix,\n",
    "                                     write_prefix= w_prefix,\n",
    "                                     read_path = DataDirectory,\n",
    "                                     write_path = DataDirectory,\n",
    "                                     parameter_dictionary=lsdtt_parameters)\n",
    "    lsdtt_drive.print_parameters()\n",
    "    lsdtt_drive.run_lsdtt_command_line_tool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b103c96-6014-428a-9dfa-ae942ca0c5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subdivision in subdivisions:\n",
    "    run_lsdtt_for_basin(subdivision, part = \"PART2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8b3c64-9a32-4cdb-929b-b53f75bb261a",
   "metadata": {},
   "source": [
    "As in Part 1, we need to make sure the segment bearings are reduced to just those in the basins and not the entire DEM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69e1e4b-a969-43a4-8281-7dd1ac66b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subdivision in subdivisions: \n",
    "    rnp.merge_segment_bearings_with_chi_data_map_for_basin(subdivision, part = \"PART2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647f9887-b00d-46bd-b995-97d1b6d5d1ac",
   "metadata": {},
   "source": [
    "Add the basin_key to the junction angles csv: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf0c30c-f4c8-41b1-9f4d-da41c987a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subdivision in subdivisions:\n",
    "    rnp.add_basin_key_to_junction_angles_csv(subdivision, part = \"PART2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f60945-59e8-4bf4-bb22-e3bd90384160",
   "metadata": {},
   "source": [
    "## Further preparation of junction angle and segment bearings csv data for analysis and plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a72f7b-564e-45eb-9a81-f37b00169d55",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Looking at the basin rasters in QGIS, there is a lot of variation in basin size. \n",
    "I only want to include basins in the analysis that are at least of order 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f4afe4-d192-49ed-9086-6808d4e6bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subdivision in subdivisions: \n",
    "    rnp.remove_low_order_basins_from_segment_bearings_csv(subdivision, part = \"PART2\", threshold=5)\n",
    "    rnp.remove_low_order_basins_from_junction_angles_csv(subdivision, part = \"PART2\", threshold=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d380c9e4-14c5-43a1-b545-9efa272c990b",
   "metadata": {},
   "source": [
    "The result is saved in csvs with 'filtered' in the name.\n",
    "\n",
    "As in part 1, the measurements from river segments that are in lakes should be removed. This is done in the same way as in part 1; if any part of a segment is in a lake, then the segment will be removed from the segment bearings dataset, and so is the upstream junction from the junction angles dataset.\n",
    "\n",
    "The resulting csv's end in `_filtered_nolakes.csv`. As before, two geojsons are produced (`segment_lines.geojson` and `segment_lines_nolakes.geojson`) so you can check if the lake filtering went as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f35aacd-1b0f-4514-ab70-a562ec603605",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for subdivision in subdivisions:\n",
    "    rnp.remove_lake_segment_bearings_and_junction_angles(subdivision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8c3151-9865-443d-b848-c10bf12f9a83",
   "metadata": {},
   "source": [
    "Now the segment bearings and junction angles csv's of the subdivisions are ready to use. Let's merge them into one segment bearings csv and one junction angles csv in the PART2 directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a839c906-097e-45ae-a529-007761b9425a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import rivernetworkpy as rnp\n",
    "rnp.create_all_segment_bearings_and_junction_angles_csvs(subdivisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534c89b7-e7b7-4e0e-b43b-505dfb1acf15",
   "metadata": {},
   "source": [
    "Note that a new column \"new_basin_key\" is added. This is to ensure each basin remains to have a unique identifier. The first digit of the new basin key indicates the subdivision it originated from (the first subdivision in the subdivisions dict being 1), and the remaining digits indicate the original basin_key.\n",
    "\n",
    "The two CSV's are now ready to be analysed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa33716-f5bb-4449-8612-def0d88fa898",
   "metadata": {},
   "source": [
    "# Data Analysis and Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d134dae8-78fb-4526-b6d7-949d75916e12",
   "metadata": {},
   "source": [
    "## Junction Angles\n",
    "Want to plot the following things in one figure:\n",
    "1. Cumulative Density Functions (CDF's) of the junction angles of each basin, of each angle (A, B1, B2) into one plot. AP CDF's should be coloured one colour, and VRP another colour.\n",
    "2. A boxplot with for each angle (A, B1, B2) three boxes: the Kuiper's V-statistic (which is calculated between two CDF's) for the groups VRP-VRP, AP-AP and AP-VRP.\n",
    "3. Cumulative Density Functions (CDF's) of the Area Ratios of each basin. AP and VRP CDF's should be coloured according to the colours in the previous two plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d946637d-be30-4dc9-9f2f-e984ef18de44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rivernetworkpy as rnp\n",
    "rnp.junction_angles()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b287b0ad-1736-4c05-97b6-832f72d7ee94",
   "metadata": {},
   "source": [
    "## Segment orientations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52a44c6-0dda-4654-aa21-6ef71c0813eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import rivernetworkpy as rnp\n",
    "rnp.segment_orientations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d30900-880f-404c-ad82-717bbf3352d3",
   "metadata": {},
   "source": [
    "## Halfdials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ccdc4c-8a03-41dd-bec7-707b7dcc4b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rivernetworkpy as rnp\n",
    "rnp.halfdials(hspace=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43570e3c-3245-48fc-a3ea-e890efe8f088",
   "metadata": {},
   "source": [
    "# Extra code used to prepare other diagrams and stats used in dissertation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99b355e-7e92-4e38-aacf-0d6a255d9c36",
   "metadata": {},
   "source": [
    "## Calculating total area of VRP bedding measurements for stats in Introduction chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a5b3e1-7a5c-45ed-baf7-b310e8d4a740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Load the shapefile\n",
    "file_path = './wvbed/WV_QUADS_VRP.shp'\n",
    "gdf = gpd.read_file(file_path)\n",
    "\n",
    "# Dissolve the polygons into a single geometry\n",
    "dissolved_gdf = gdf.dissolve(aggfunc='sum') \n",
    "\n",
    "# Convert the geometry to EPSG:32617 - WGS 84 / UTM zone 17N for Cartesian area calculation\n",
    "dissolved_gdf = dissolved_gdf.to_crs(epsg=32617)\n",
    "\n",
    "# Calculate the Cartesian area in square kilometers\n",
    "dissolved_gdf['cartesian_area_km2'] = dissolved_gdf['geometry'].area / 1e6  # Convert square meters to square kilometers\n",
    "\n",
    "# Print the result\n",
    "print(\"The resurveyed quads of west virginia have a total area of: \" + str(dissolved_gdf['cartesian_area_km2'].tolist()[0]) + \"km2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf71a928-28d7-44ba-89dc-8b4d3aa42773",
   "metadata": {},
   "source": [
    "## For methods diagram: get empirical cumulative probability function graphs for some VRP and AP basins, and get hypothesised distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d917a0-9266-4a51-97fb-0f9bddeb8161",
   "metadata": {},
   "source": [
    "Make folder to store these graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45d2f98-051e-44b9-b0ef-1a15cdea125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "if os.path.exists(\"./methodsfig\") == False:\n",
    "    os.mkdir(\"./methodsfig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50c8f45-f17b-4060-912c-7ae0a65c8b35",
   "metadata": {},
   "source": [
    "Get CDF of 1 emperical and 1 generated distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef4cc8b-29f1-43a4-aa06-808c54056fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import rivernetworkpy as rnp\n",
    "\n",
    "# load data\n",
    "sb_df = pd.read_csv(\"./basins/PART2/all_segment_bearings.csv\")\n",
    "    \n",
    "# remove directionality from bearings, i.e. make orientations column on [0,180)\n",
    "sb_df[\"segment_orientation\"] = rnp.remove_directionality(sb_df[\"segment_bearing_whole_segment\"].tolist())\n",
    "\n",
    "# get orientations\n",
    "orientation_distribution = sb_df[sb_df[\"new_basin_key\"]==25][\"segment_orientation\"].to_numpy()\n",
    "\n",
    "# get hypothesised distribution\n",
    "hypothesised_distribution = rnp.generate_distribution(p1=1, peak=0, distribution=\"bimodal\", s = 30, m = 90, n = 1000)\n",
    "\n",
    "# put in list\n",
    "distributions = [orientation_distribution, hypothesised_distribution]\n",
    "\n",
    "# Create a common set of values for the x-axis\n",
    "x_values = np.linspace(0, 180, 1000)\n",
    "\n",
    "# create figure\n",
    "plt.figure(figsize=(4,4))\n",
    "\n",
    "# select colours\n",
    "colours = [\"darkorange\", \"cornflowerblue\"]\n",
    "\n",
    "for distribution, colour in zip(distributions, colours):\n",
    "    # Calculate the empirical cumulative distribution function (ECDFs)\n",
    "    ecdf = np.searchsorted(np.sort(distribution), x_values, side='right') / len(distribution)\n",
    "    \n",
    "    # Plot ecdf\n",
    "    plt.step(x_values, ecdf, where='post', color = colour)\n",
    "\n",
    "plt.xlim((0,180))\n",
    "plt.xticks([0,45,90,135,180], [\"N\", \"NE\", \"E\", \"SE\", \"S\"])\n",
    "\n",
    "plt.ylim((0,1))\n",
    "plt.yticks([0,1])\n",
    "plt.xlabel('Orientation')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.savefig(\"./methodsfig/ECDF_CDF.jpg\", dpi=360)\n",
    "\n",
    "# get V value (note this code is identical to the Calculate Kuipers V function in Rivernetworkpy BUT now also get D+ and D-\n",
    "# Calculate the empirical cumulative distribution functions (ECDFs)\n",
    "ecdf1 = np.searchsorted(np.sort(orientation_distribution), x_values, side='right') / len(orientation_distribution)\n",
    "ecdf2 = np.searchsorted(np.sort(hypothesised_distribution), x_values, side='right') / len(hypothesised_distribution)\n",
    "\n",
    "# Calculate the differences between ECDFs\n",
    "differences = ecdf1 - ecdf2\n",
    "\n",
    "# Calculate the maximum positive and maximum negative differences\n",
    "max_positive_difference = np.max(np.clip(differences, 0, None))\n",
    "max_negative_difference = np.abs(np.min(np.clip(differences, None, 0)))\n",
    "\n",
    "# get location of D+ and D-\n",
    "max_positive_index = round(np.argmax(differences)/1000*180)\n",
    "max_negative_index = round(np.argmax(-differences)/1000*180)\n",
    "\n",
    "# Calculate the combined value V\n",
    "v = max_positive_difference + max_negative_difference\n",
    "print(\"D+, D- and V are:\", max_positive_difference,max_negative_difference,v)\n",
    "print(\"D+ and D- are located at\", max_positive_index, max_negative_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2573662d-3c42-42c6-9013-b27e50202cbe",
   "metadata": {},
   "source": [
    "Now get empirical cdf's for 1 VRP and 1 AP basin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8adb16a-6ce8-4d92-832a-ccb459d2b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import rivernetworkpy as rnp\n",
    "\n",
    "# load data\n",
    "sb_df = pd.read_csv(\"./basins/PART2/all_segment_bearings.csv\")\n",
    "    \n",
    "# remove directionality from bearings, i.e. make orientations column on [0,180)\n",
    "sb_df[\"segment_orientation\"] = rnp.remove_directionality(sb_df[\"segment_bearing_whole_segment\"].tolist())\n",
    "\n",
    "# select basins by new_basin_key\n",
    "new_basin_keys = [25, 735]\n",
    "\n",
    "# Create a common set of values for the x-axis\n",
    "x_values = np.linspace(0, 180, 1000)\n",
    "\n",
    "# create figure\n",
    "plt.figure(figsize=(4,4))\n",
    "\n",
    "for new_basin_key in new_basin_keys:\n",
    "    # get orientations\n",
    "    orientations = sb_df[sb_df[\"new_basin_key\"]==new_basin_key][\"segment_orientation\"].to_numpy()\n",
    "\n",
    "    # Calculate the empirical cumulative distribution function (ECDFs)\n",
    "    ecdf = np.searchsorted(np.sort(orientations), x_values, side='right') / len(orientations)\n",
    "\n",
    "    # get colour:\n",
    "    color = \"darkorange\" if (int(str(new_basin_key)[0]) < 5) else \"limegreen\"\n",
    "    \n",
    "    # Plot ecdf\n",
    "    plt.step(x_values, ecdf, where='post', color = color)\n",
    "\n",
    "plt.xlim((0,180))\n",
    "plt.xticks([0,45,90,135,180], [\"N\", \"NE\", \"E\", \"SE\", \"S\"])\n",
    "\n",
    "plt.ylim((0,1))\n",
    "plt.yticks([0,1])\n",
    "plt.xlabel('Orientation')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.savefig(\"./methodsfig/VRP_AP_BASIN_ECDF.jpg\", dpi=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a240d11f-9609-49bb-bb57-b170fff4b7f3",
   "metadata": {},
   "source": [
    "## Code for second methods diagram, the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41fdb84-3b01-4da5-a053-447021fae2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rivernetworkpy as rnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure(figsize=(9,6))\n",
    "gs_outer = gridspec.GridSpec(2,1)\n",
    "gs_upper = gridspec.GridSpecFromSubplotSpec(1,3, subplot_spec=gs_outer[0]) # for the distributions of the fist type\n",
    "gs_lower = gridspec.GridSpecFromSubplotSpec(1,3, subplot_spec=gs_outer[1]) # for the distributions of the second type\n",
    "\n",
    "# Create a common set of values for the x-axis\n",
    "x_values = np.linspace(0, 180, 1000)\n",
    "\n",
    "# set linestyles and colours \n",
    "ls_list = [\"--\",\"--\"]\n",
    "colours = [\"cornflowerblue\", \"grey\"]\n",
    "\n",
    "for gsnumber in [0,1,2]:\n",
    "    # get p from gsnumber\n",
    "    p1 = gsnumber/2\n",
    "\n",
    "    # plot the distribution of the first type for this p\n",
    "    ax_1 = plt.subplot(gs_upper[gsnumber])\n",
    "    distributions_1 = [rnp.generate_distribution(p1=p1, peak=0, distribution=\"bimodal\", s = 30, m = 90, n = 1000), rnp.generate_distribution(p1=p1, peak=45, distribution=\"bimodal\", s = 30, m = 90, n = 1000)]\n",
    "    for distribution, ls, colour in zip(distributions_1, ls_list, colours):\n",
    "        # Calculate the cumulative distribution function \n",
    "        cdf = np.searchsorted(np.sort(distribution), x_values, side='right') / len(distribution)\n",
    "        # Plot ecdf\n",
    "        ax_1.step(x_values, cdf, where='post', color = colour, ls=ls)\n",
    "\n",
    "    # plot the distribution of the second type for this p\n",
    "    ax_2 = plt.subplot(gs_lower[gsnumber])\n",
    "    distributions_2 = [rnp.generate_distribution(p1=p1, peak=0, distribution=\"skewed_bimodal\", s = 30, m = 90, n = 1000), rnp.generate_distribution(p1=p1, peak=45, distribution=\"skewed_bimodal\", s = 30, m = 90, n = 1000)]\n",
    "    for distribution, ls, colour in zip(distributions_2, ls_list, colours):\n",
    "        # Calculate the cumulative distribution function \n",
    "        cdf = np.searchsorted(np.sort(distribution), x_values, side='right') / len(distribution)\n",
    "        # Plot ecdf\n",
    "        ax_2.step(x_values, cdf, where='post', color = colour, ls=ls)\n",
    "\n",
    "    # do plot layout\n",
    "    for ax in [ax_1, ax_2]:\n",
    "        ax.set_ylim((0,1))\n",
    "        ax.set_xlim((0,180))\n",
    "        ax.set_yticks([0,1])\n",
    "        ax.set_xticks([0,45,90,135,180],[\"N\",\"NE\",\"E\", \"SE\",\"S\"])\n",
    "\n",
    "plt.savefig(\"./methodsfig/distributions.jpg\", dpi=360)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
